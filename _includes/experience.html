<section class="section experiences-section">
    <h2 class="section-title"><i class="fa fa-briefcase"></i>Experience</h2>

    <div class="item">
        <div class="meta">
            <div class="upper-row">
                <h3 class="job-title">Senior Site Reliability Engineer</h3>
                <div class="time">2017 - Current</div>
            </div><!--//upper-row-->
            <div class="company">BuzzFeed, London &amp; New York</div>
        </div><!--//meta-->
        <div class="details">
            <p>Senior Site Reliability Engineer at BuzzFeed (Infrastructure/Resilience squad), bolstering the SRE team's Software Engineering skill set. Owning and developing the AWS and GCP based internal PaaS to match the growing needs of the rest of BuzzFeed tech and automating myself into more exciting problems.</p>

            <h5>Projects</h5>
            <ul class="project-list">
                <li>Designed and implemented automated assignment of cloud-provider IAM roles to individual microservices, improving security by locking down service access to data stores and paving the way for subsequent work around service-to-service identity.</li>
                <li>Designed and implemented service-to-service authentication and authorisation mechanism based on IAM identities (inspired by Vault's IAM auth implementation), allowing service owners to control fine-grained application-level permissions between services.</li>
                <li>Re-architected AWS load balancing to make more efficient use of Application Load Balancers, improving deployment speeds, operational resilience and saving $13k per month.</li>
                <li>Re-architected service migration allowing zero downtime transitions within the internal PaaS for wider sets of configuration changes.</li>
                <li>Led a BuzzFeed tech-wide developer experience engagement exercise, testing a set of user personas and problem hypotheses to check we understood user needs, and collect targeted feedback about how to improve day-to-day development pain, and what long-term capabilities we needed to target.</li>
                <li>Automating multiple aspects of the Terraform setup of new instances of the PaaS for better disaster recovery, increased automation and improved testing.</li>
                <li>Designed new architecture to improve cluster isolation within PaaS, allowing clusters to be more easily deployed across multiple AWS accounts.</li>
            </ul>

            <h5>Tools &amp; Techs</h5>
            <p>AWS and GCP (Compute, Storage, ML/DataSci and Infrastructure services), Docker/ECS(+FarGate)/GKE, DataDog, PaperTrail/DataDog Logs, CloudFormation/Terraform, Jenkins, Python (all PaaS tooling and frameworks), Go (API gateway and SSO services).</p>
        </div><!--//details-->
    </div><!--//item-->

    <hr></hr>

    <div class="item">
        <div class="meta">
            <div class="upper-row">
                <h3 class="job-title">Senior Software Engineer</h3>
                <div class="time">2016 - 2017</div>
            </div><!--//upper-row-->
            <div class="company">OVO Energy, London</div>
        </div><!--//meta-->
        <div class="details">
            <p>Senior and Acting-Principal developer on the OVO Live product, supporting the real-time collection of high granularity (10-second) Energy usage data (thousands of distributed devices reporting every few seconds) and provide insights to customers and the rest of the business. The team also covered the collection and insight into smart-meter data (30min and 1day granularity).</p>

            <h5>Projects</h5>
            <ul class="project-list">
                <li>Owned, operated and developed Kafka + Akka powered ingestion pipeline for storing real-time energy consumption data (13k devices reporting gas and electricity usage every 10 seconds). Data stored in 1 minute intervals in InfluxDB cluster.</li>
                <li>Owned, operated and developed Kafka + Akka powered ingestion pipeline for storing smart energy consumption data (700k devices reporting 30-min and 24-hour granularity data, batched from third party every day). Data stored in Cassandra cluster.</li>
                <li>Architected and implemented a pipeline for sharing anonymised smart meter usage data with a third party to collect disaggregated energy usage (e.g., what portion of energy went towards heating, lighting, entertainment devices).</li>
                <li>Owned and developed customer-facing frontend for viewing 1-minute, 30-minute and daily energy usage statistics, and comparison to their previous usage.</li>
                <li>Designed and developed new customer-facing frontend for viewing a categorised breakdown of energy usage.</li>
                <li>Captured all existing infrastructure as code, and developed a small framework for verifying and deploying infrastructure changes to be used as part of our CI pipeline, allowing us to evolve to Continuous Delivery.</li>
                <li>Helped teams bootstrap usage of GCP/GKE to make more efficient use of BigQuery for business insight services/tooling.</li>
                <li>Led the creation of a cross-team task force to tackle inconsistencies in global Customer UX, resulting in the creation of a new team to specifically tackle this problem area, and create a global style guide to make it easier for teams to be consistent.</li>
            </ul>

            <h5>Tools &amp; Techs</h5>
            <p>Scala, Akka (Streams, Kafka, HTTP), Kafka, Spark, InfluxDB, Cassandra, Grafana, Kapacitor, Angular, React, AWS (Compute, Storage, Messaging, Infrastructure), GCP (GKE, BigQuery), Docker/ECS/GKE, GoCD, Jenkins, Terraform/CloudFormation, Python (CI/CD library)</p>
        </div><!--//details-->
    </div><!--//item-->

    <hr></hr>

    <div class="item">
        <div class="meta">
            <div class="upper-row">
                <h3 class="job-title">Senior Software Engineer</h3>
                <div class="time">2015 - 2016</div>
            </div><!--//upper-row-->
            <div class="company">BBC, MediaCityUK</div>
        </div><!--//meta-->
        <div class="details">
            <p>Working under the Content Discovery as part of the BBC Online Search team, providing APIs and UIs for searching content produced across the entire BBC. The APIs and UIs supported every domestic BBC site/sub-site, iPlayer Video/Radio and newer mobile apps.</p>

            <h5>Projects</h5>
            <ul class="project-list">
                <li>Architected and implemented year-long migration from commercial search-engine, hosted in-house, to open source (Solr) hosted in AWS with zero downtime or change in functionality to consumers across the entire business.</li>
                <li>Re-architected, designed and implemented new search, suggestion and recommendation UI to become part of the site-wide BBC header, establishing new team norms for frontend development using React.</li>
                <li>Trained with AWS to become a self-reliant DevOps team, owning and operating the newly migrated Search systems. This empowered us to improve costs, resilience (more flexible and automated scaling), and business functionality (ability to quickly create new sub-indexes of search content for specific audiences).</li>
                <li>Architected and implemented a serverless pipeline for ingesting documents from any team in the BBC to be indexed in the new search system. Kept existing API contracts in place but completely re-architected everything underneath, giving us the flexibility to add new sources/destinations for indexed content, and practically unlimited scalability for bursty workloads.</li>
                <li>As a 10% R&amp;D project, collaborated to create a "toolshed" project which used Docker/ECS to allow our team (and other teams in the same group) to rapidly deploy prototypes, proof-of-concepts or short-lived systems without the need to do heavyweight deployment of servers as had previously been the norm.</li>
                <li>Collaborated with other teams in the group to develop an internal python framework to interface onto the BBC's federated cloud APIs to allow us to enable Continuous Deployment all the way to Prod from our Jenkins pipelines.</li>
            </ul>

            <h5>Tools &amp; Techs</h5>
            <p>Scala, Python, Java, Ruby, Spring, Camel, Rails, Cucumber, Solr, Elasticsearch, Exalead Cloudview (COTS search engine), AWS (Compute, Storage, Messaging, Infrastructure, Identity), Node (Express, React), Jenkins.</p>
        </div><!--//details-->
    </div><!--//item-->

    <hr></hr>

    <div class="item">
        <div class="meta">
            <div class="upper-row">
                <h3 class="job-title">Software Engineer</h3>
                <div class="time">2014 - 2015</div>
            </div><!--//upper-row-->
            <div class="company">BBC, MediaCityUK</div>
        </div><!--//meta-->
        <div class="details">
                <p>Worked under the Content Discovery team on feature development across Homepage and Search services (Java, Scala, Node.js). Spent significant time to automate deployments and testing. Team norms were to use Cucumber for BDD which ran under Ruby, and language appropriate libraries for TDD (e.g., JUnit in Java).</p>

                <p>Began to introduce Docker and Vagrant into the projects to provide an automated mechanism for bootstrapping development environments which provided repeatable and portable deployments to improve testing, and also new team members to be on-boarded more effectively.</p>
        </div><!--//details-->
    </div><!--//item-->

    <hr></hr>

    <div class="item">
        <div class="meta">
            <div class="upper-row">
                <h3 class="job-title">Postdoctoral Research Associate</h3>
                <div class="time">2011 - 2014</div>
            </div><!--//upper-row-->
            <div class="company">Newcastle University, Newcastle upon Tyne</div>
        </div><!--//meta-->
        <div class="details">
                <p>Part of a research group within Newcastle University called the Digital Institute collaborating with industry and other academic units to provide software development and consulting.</p>

                <p>Main development focussed on a JavaEE based scientific workflow processing system supporting the generation and tracing of experimental data with provenance, allowing derivative experiment use to be tracked and audited.</p>

                <p>Collaborative project with IBM on migrating psychological model evaluation from a desktop environment to a massively parallelised cloud environment (AWS), allowing them to effectively test tens of thousands of variations in seconds.</p>

                <p>Academic responsibilities including contributing and leading research papers, lecturing to undergrad and postgrad students, and PhD mentoring.</p>
        </div><!--//details-->
    </div><!--//item-->

</section><!--//section-->
