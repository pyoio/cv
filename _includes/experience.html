<section class="section experiences-section">
    <h2 class="section-title"><i class="fa fa-briefcase"></i>Experience</h2>

    <div class="item">
        <div class="meta">
            <div class="upper-row">
                <h3 class="job-title">Senior Site Reliability Engineer</h3>
                <div class="time">2017 - Current</div>
            </div><!--//upper-row-->
            <div class="company">BuzzFeed, London &amp; New York</div>
        </div><!--//meta-->
        <div class="details">
            <p>Senior Site Reliability Engineer at BuzzFeed, helping bolster the SRE team's Software Engineering skill set. Owning and developing the internal platform as a service to match the growing needs of the rest of BuzzFeed tech and automating myself into more interesting problems.</p>

            <p>The platform is primarily hosted in AWS, using DataDog for logging/metrics. We make use of all the usual suspects in AWS: ECS/ECR, Lambda, S3, RDS. Architecture is captured and deployed using Terraform. More recently we have been using GCP's BigQuery and managed ML offerings to assist Data Science and other teams using ML (e.g, recommendations).</p>

            <p>Most recently I did work to automate the creation of the platform and its clusters from scratch, and improved isolation within the code and architecture to allow these new clusters to be constructed in separate AWS accounts. This helped us avoid AWS API limits, improving security and resilience (via isolation) and increased genereal development speed by automating away more manual aspects including testing.</p>

            <p>Prior to that I led SRE's first engagement exercise with tech-as-a-whole, to test that we understood their current and future needs. We did this by creating multiple personas and hypotheses and interviewed multiple people in each persona to collect targeted feedback. Using the feedback we collated a set of problem areas to tackle for some immediate wins in developer productivity/pain, and some longer term goals that aligned with higher level business strategy (e.g., work to enable active-active multi-region).</p>

            <p>Throughout my entire role I have been a vocal proponent of improving the automation and testing of all aspects of our internal platform (the underlying code itself, and the core services we provide within it). This has allowed us to speed up the feedback loop and gain increased confidence in the changes we make.</p>
        </div><!--//details-->
    </div><!--//item-->

    <div class="item">
        <div class="meta">
            <div class="upper-row">
                <h3 class="job-title">Senior/Principal Software Engineer</h3>
                <div class="time">2016 - 2017</div>
            </div><!--//upper-row-->
            <div class="company">OVO Energy, London</div>
        </div><!--//meta-->
        <div class="details">
            <p>Engineer at OVO Energy leading development on the Live product, whose mission is to support the real-time collection of high granularity Energy usage data (thousands of distributed devices reporting every few seconds) and provide insights to customers and the rest of the business.</p>

            <p>Core stack consists of Scala, Akka (streams, kafka, http) services, Kafka, Spark, InfluxDB, Cassandra, Grafana, Kapacitor, and AWS DynamoDB, S3, SQS, and SNS. All deployed into AWS, infrastructure captured in code and automation at all levels, fierce proponent of Agile, XP, DevOps, and TDD/BDD. All services deployed as containers using CloudFormation and AWS ECS/ECR.</p>

            <p>I lead efforts to help teams migrate into their own AWS accounts and automate provisioning of infrastructure using TerraForm and AWS CloudFormation depending on the teams tooling choices. I also led workshops on effective operation of containerised JVM applications in a multi-tenant computing scenario.</p>

            <p>I was involved in the bootstrapping and proof of concept work around an internal data lake project, allowing multiple teams to easily stream their data (Kafka events with a schema) into BigQuery and make it available for insight throughout the business. This also involved the use of Google Cloud's Kubernetes Engine (GKE).</p>

            <p>Involved in the creation of a squad across multiple teams to address cross-cutting concerns with the overall customer experience, ultimately resulting in the creation of a new team whose remit was holistic oversight of the customer experience, and improved feedback from customers.</p>

            <p>Collaborated to bootstrap internal data platform in Google Container Service, leveraging my experience with containerisation and automation to easily capitalise on the flexibility offered by Kubernetes.</p>
        </div><!--//details-->
    </div><!--//item-->

    <div class="item">
        <div class="meta">
            <div class="upper-row">
                <h3 class="job-title">Senior Software Engineer</h3>
                <div class="time">2015 - 2016</div>
            </div><!--//upper-row-->
            <div class="company">BBC, MediaCityUK</div>
        </div><!--//meta-->
        <div class="details">
                <p>Senior Developer at the BBC working under the Content Discovery group supporting the BBC Homepage, site wide navigation, and BBC Online Search (used by every BBC site/app including iPlayer Video/Radio).</p>

                <p>Lead on technology choices and implementation of a complete migration of the Search systems from an in house commercial platform to an open source solution (Apache Solr) hosted in AWS. We worked closely with AWS to adopt best practices for architecting scalable and resilient systems in the cloud.</p>

                <p>Implemented a new Cloud based pipeline for ingesting and indexing content from production and editorial teams around the BBC. Implemented using serverless principles (AWS lambda) giving us great scalability and cost management. This pipeline allows us to add new sources and destinations with ease, and without any interruption of service. We worked with AWS to adopt best CD practices for use of AWS lambda and I also created a dockerised test harness for us to test the lambda locally.</p>

                <p>As a 10% R&D project I collaborated to roll out a "toolshed" project which allowed us to rapidly develop prototypes in Docker containers and deploy them into the toolshed to be evaluated, or used where needed (e.g., dashboards, prototype tools). I also worked across multiple projects to provide a Python libraries to help us create repeatable and portable deployment pipelines, and promoted the use of Jenkins pipelines.</p>
        </div><!--//details-->
    </div><!--//item-->

    <div class="item">
        <div class="meta">
            <div class="upper-row">
                <h3 class="job-title">Software Engineer</h3>
                <div class="time">2014 - 2015</div>
            </div><!--//upper-row-->
            <div class="company">BBC, MediaCityUK</div>
        </div><!--//meta-->
        <div class="details">
                <p>Worked under the Content Discovery team on feature development across Homepage and Search services (Java, Scala, Node.js). Spent a lot of time automating deployments and testing. Team norms were to use Cucumber for BDD which ran under Ruby, and language appropriate libraries for TDD (e.g., JUnit in Java).</p>

                <p>Began to introduce Docker and Vagrant into the projects to provide an automated mechanism for bootstrapping development environments which provided repeatable and portable deployments to improve testing, and also new team members to be on-boarded more effectively.</p>
        </div><!--//details-->
    </div><!--//item-->

    <div class="item">
        <div class="meta">
            <div class="upper-row">
                <h3 class="job-title">Postdoctoral Research Associate</h3>
                <div class="time">2011 - 2014</div>
            </div><!--//upper-row-->
            <div class="company">Newcastle University, Newcastle upon Tyne</div>
        </div><!--//meta-->
        <div class="details">
                <p>Part of a research group within Newcastle University called the Digital Institute collaborating with industry and other academic units to provide software development and consulting.</p>

                <p>Main development focussed on a JavaEE based scientific workflow processing system supporting the generation and tracing of experimental data with provenance, allowing derivative experiment use to be tracked and audited.</p>

                <p>Collaborative project with IBM on migrating psychological model evaluation from a desktop environment to a massively parallelised cloud environment (AWS), allowing them to effectively test tens of thousands of variations in seconds.</p>

                <p>Academic responsibilities including contributing and leading research papers, lecturing to undergrad and postgrad students, and PhD mentoring.</p>
        </div><!--//details-->
    </div><!--//item-->

</section><!--//section-->
